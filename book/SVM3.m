% This tests the support vector machine content that appears in the book
% After running this, h will contain the figure handle of the plot that is
% created.  If two plots are created, h1 and h2 are the figure handles.

% To allow for the low-rank expansion parameter to be set
global GAUSSQR_PARAMETERS

% Initial example for support-vector machines
if exist('rng','builtin')
    rng(0);
else
    rand('state',0);
    randn('state',0);
end

% Define our Gaussian RBF
rbf = @(e,r) exp(-(e*r).^2);

% Choose a range of ep and bc values
epvec = logspace(-2,2,30);
bcvec = logspace(-2,4,31);

% Choose the number of cross-validations to compute
cv_fold = 10;

% Choose whether to compute 1D or 2D plots
plot2D = 1;

% Use the low rank matrix multiplication strategy
low_rank = 0;
GAUSSQR_PARAMETERS.DEFAULT_REGRESSION_FUNC = .05;

% Define our normal distributions
grnmean = [1,0];
redmean = [0,1];
grncov = eye(2);
redcov = eye(2);

% How many points of each model do we want to classify and learn from
grn_test_N = 10;
red_test_N = 10;
grn_train_N = 100;
red_train_N = 100;

% How much fudge factor do we want in our training set
grn_buffer = .2;
red_buffer = .2;

% Generate some manufactured data and attempt to classify it
% The data will be generated by normal distributions with different means
% Half of the data will come from [1,0] and half from [0,1]
grnpop = mvnrnd(grnmean,grncov,grn_test_N);
redpop = mvnrnd(redmean,redcov,red_test_N);

% Generate a training set from which to learn the classifier
grnpts = zeros(grn_train_N,2);
redpts = zeros(red_train_N,2);
for i = 1:grn_train_N
    grnpts(i,:) = mvnrnd(grnpop(ceil(rand*grn_test_N),:),grncov*grn_buffer);
end
for i = 1:red_train_N
    redpts(i,:) = mvnrnd(redpop(ceil(rand*red_test_N),:),redcov*red_buffer);
end

% Create a vector of data and associated classifications
% Green label 1, red label -1
train_data = [grnpts;redpts];
train_class = ones(grn_train_N+red_train_N,1);
train_class(grn_train_N+1:grn_train_N+red_train_N) = -1;
N_train = length(train_class);
test_data = [grnpop;redpop];
test_class = ones(grn_test_N+red_test_N,1);
test_class(grn_test_N+1:grn_test_N+red_test_N) = -1;
N_test = length(test_class);

if plot2D
    % Produce a 2D plot for a range of ep and bc values
    cvmat = zeros(length(epvec),length(bcvec));
    errmat = zeros(length(epvec),length(bcvec));
    k_ep = 1;
    h_waitbar = waitbar(0,'Initializing');
    for ep=epvec
        k_bc = 1;
        for bc=bcvec
            cvmat(k_ep,k_bc) = gqr_svmcv(cv_fold,train_data,train_class,ep,bc,low_rank);
            SVM = gqr_fitsvm(train_data,train_class,ep,bc,low_rank);
            errmat(k_ep,k_bc) = sum(test_class ~= SVM.eval(test_data));
            
            progress = floor(100*((k_ep-1)*length(bcvec)+k_bc)/(length(epvec)*length(bcvec)))/100;
            waitbar(progress,h_waitbar,sprintf('%d-fold CV, \\epsilon=%5.2f C=%5.2f',cv_fold,ep,bc))
            k_bc = k_bc + 1;
        end
        k_ep = k_ep + 1;
    end
    
    waitbar(1,h_waitbar,'Plotting')
    [E,B] = meshgrid(epvec,bcvec);
    
    h1 = figure;
    h_ev = surf(E,B,cvmat');
    set(h_ev,'edgecolor','none')
    set(gca,'xscale','log')
    set(gca,'yscale','log')
    set(gca,'ytick',[1e-2,1e1,1e4])
    xlabel('\epsilon')
    ylabel('C')
    zlim([.7,1.1])
    zlabel(sprintf('%d-fold CV residual',cv_fold))
    shading interp
    grid off
    view([-.7,1,1])
    
    h2 = figure;
    h_err = surf(E,B,errmat');
    set(h_err,'edgecolor','none')
    set(gca,'xscale','log')
    set(gca,'yscale','log')
    set(gca,'ytick',[1e-2,1e1,1e4])
    set(gca,'ztick',[0,5,10])
    xlabel('\epsilon')
    ylabel('C')
    zlim([0,10])
    zlabel(sprintf('missed classifications',cv_fold))
    shading interp
    grid off
    view([-.7,1,1])
    
    close(h_waitbar)
else
    % Produce a plot studying the associated CV residuals with one parameter
    % fixed and the other allowed to vary
    % We could allow for other ep and bc values, but whatever
    errvec_ep = zeros(size(epvec));
    errvec_bc = zeros(size(bcvec));
    k = 1;
    h_waitbar = waitbar(0,'Initializing');
    bc = 1;
    for ep=epvec
        errvec_ep(k) = gqr_svmcv(cv_fold,train_data,train_class,ep,bc,low_rank);
        k = k + 1;
        progress = floor(100*k/(length(bcvec)+length(epvec)))/100;
        waitbar(progress,h_waitbar,'Computing \epsilon CV')
    end
    k = 1;
    ep = 1;
    for bc=bcvec
        errvec_bc(k) = gqr_svmcv(cv_fold,train_data,train_class,ep,bc,low_rank);
        k = k + 1;
        progress = floor(100*(k+length(epvec))/(length(bcvec)+length(epvec)))/100;
        waitbar(progress,h_waitbar,'Computing C CV')
    end
    %         bc = 1;
    %         waitbar(progress,h_waitbar,'Optimality Search')
    %         [ep_opt,cv_opt] = fminbnd(@(ep)gqr_svmcv(cv_fold,train_data,train_class,ep,bc),.1,10);
    %         [cv_opt,opt_ind] = min(errvec_ep);
    %         ep_opt = epvec(opt_ind);
    waitbar(1,h_waitbar,'Plotting')
    h = figure;
    semilogx(epvec,errvec_ep,'linewidth',2)
    hold on
    semilogx(bcvec,errvec_bc,'--','linewidth',2)
    ylabel(sprintf('%d-fold CV residual',cv_fold))
    legend('C=1, \epsilon x-axis','\epsilon=1, C x-axis','location','northwest')
    hold off
    close(h_waitbar)
end